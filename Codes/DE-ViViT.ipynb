{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPK2ix7tVIVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "#import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import glob\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "import math \n",
    "# Setting seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)\n",
    "random.seed(SEED)\n",
    "RESULT_PATH = \"results_m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 24\n",
    "BATCH_SIZE = 32\n",
    "FRAME = 10\n",
    "CHANNEL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 24\n",
    "BATCH_SIZE = 32\n",
    "FRAME = 10\n",
    "CHANNEL = 5\n",
    "\n",
    "\n",
    "train_flist = [] \n",
    "test_flist = [] \n",
    "val_flist = [] \n",
    "\n",
    "\n",
    "for cpath, dirs, files in os.walk('HUST-LEBW/training'):\n",
    "    if '.jpg' in ''.join(files):\n",
    "        train_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath]\n",
    "            \n",
    "for cpath, dirs, files in os.walk('HUST-LEBW/test'):\n",
    "    if  '.jpg' in ''.join(files):\n",
    "        val_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath]\n",
    "        \n",
    "for cpath, dirs, files in os.walk('MAEB'):\n",
    "    if '.jpg' in ''.join(files):\n",
    "        test_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath and np.sum(cv.imread(cpath + '/' + f)) !=0 ]\n",
    "            \n",
    "random.shuffle(train_flist)\n",
    "\n",
    "test_flist = test_flist + val_flist\n",
    "\n",
    "\n",
    "train_lb = [ 1 if f.split('/')[2] == 'blink' or f.split('/')[2] == 'close' or f.split('/')[3] == 'close' else 0 for f in train_flist ]\n",
    "val_lb = [ 1 if f.split('/')[2] == 'blink' or f.split('/')[2] == 'close' or f.split('/')[3] == 'close' else 0 for f in val_flist ]\n",
    "test_lb = [ 1 if f.split('/')[2] == 'blink' or f.split('/')[2] == 'close' or f.split('/')[3] == 'close' else 0 for f in test_flist ]\n",
    "\n",
    "len(train_flist), len(val_flist),len(test_flist), len(train_lb), len(val_lb), len(test_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qxi8ENIVIVF"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, FRAME, CHANNEL)\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 4, 3)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFK348gIVIVH"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_img(path, mode='train', trans = [0, 2, 1, 3]):\n",
    "    \n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "\n",
    "    if mode =='train':\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        \n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE*FRAME))\n",
    "    img = tf.reshape(img, (IMG_SIZE, FRAME,IMG_SIZE, 1))\n",
    "    img = tf.transpose(img, trans)\n",
    "    \n",
    "    # Make an input of residual embedding\n",
    "    img2 = (img - img[:,:,0:1, :])\n",
    "    img3 = (img - img[:,:,9:, :])\n",
    "    img4 = (img - img[:,:,4:5, :])\n",
    "    img5 = (img - img[:,:,5:6, :])\n",
    "\n",
    "    return tf.concat([img, img2, img3, img4, img5], axis=-1)\n",
    "\n",
    "def get_dataset(flist , lbs, mode = 'train', trans = [0 , 2, 1, 3]): # [1, 0, 2, 3]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( flist )\n",
    "    dataset_lb = tf.data.Dataset.from_tensor_slices( lbs )\n",
    "    \n",
    "    dataset = dataset.map(lambda x: get_img(x, mode, trans), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = tf.data.Dataset.zip((dataset, dataset_lb))\n",
    "\n",
    "    if mode == 'train':\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)      \n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = math.ceil(len(train_flist) / BATCH_SIZE)\n",
    "test_steps_per_epoch = math.ceil(len(test_flist) / BATCH_SIZE)\n",
    "val_steps_per_epoch = math.ceil(len(val_flist) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyVm78pSVIVJ",
    "tags": []
   },
   "source": [
    "## Video Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zrh4Ot1IVIVJ"
   },
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = tf.keras.layers.Conv3D(32, (3,3,3), padding='same', activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPool3D((2,2,1))\n",
    "        self.projection2 = tf.keras.layers.Conv3D(64, (3,3,3), padding='same', activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPool3D((2,2,1))\n",
    "        self.projection3 = tf.keras.layers.Conv3D(128, (3,3,3), padding='same')\n",
    "        self.pool4 = tf.keras.layers.AveragePooling3D((3,3,1), strides=(3,3,1))\n",
    "\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        \n",
    "        x = self.projection(videos)    \n",
    "        x = self.pool(x)\n",
    "        x = self.projection2(x)    \n",
    "        x = self.pool2(x)\n",
    "        x = self.projection3(x)    \n",
    "        x = self.pool4(x)\n",
    "\n",
    "        flattened_patches = self.flatten(x)\n",
    "        return flattened_patches\n",
    "\n",
    "    \n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _,  num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n",
    "    \n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    tubelet_embedder2,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    \n",
    "    patches1 = tubelet_embedder(inputs[:,:,:,:, 0:1])# Tubelet embedding's output\n",
    "    patches2 = tubelet_embedder2(inputs[:,:,:,:, 1:])# Residual embedding's output\n",
    "    \n",
    "    patches = tf.keras.layers.Concatenate()([patches1, patches2])\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    atten = []\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output, attention_output_score = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=(2*embed_dim) // num_heads, dropout=0.1, \n",
    "        )(x1, x1, return_attention_scores=True)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim*2, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes)(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YtbqCVOVIVK"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLgn83s2VIVK"
   },
   "outputs": [],
   "source": [
    "def vit(fid):\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM\n",
    "        ),\n",
    "        tubelet_embedder2=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM*2),\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset(train_flist, train_lb, mode = 'train', trans = [0 , 2, 1, 3])\n",
    "    test_dataset = get_dataset(test_flist, test_lb, mode = 'test', trans =[0 , 2, 1, 3])\n",
    "    val_dataset = get_dataset(val_flist, val_lb, mode = 'test',   trans =[0 , 2, 1, 3])\n",
    "\n",
    "    SEED = 42\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "    keras.utils.set_random_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='model/VIT_{}'.format(fid),save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(train_dataset, steps_per_epoch=train_steps_per_epoch, epochs=EPOCHS, validation_data=val_dataset, validation_steps=val_steps_per_epoch, callbacks=[model_checkpoint_callback],)\n",
    "\n",
    "    best_model = keras.models.load_model('model/VIT_{}'.format(fid))\n",
    "\n",
    "    preds = tf.nn.sigmoid(best_model.predict(test_dataset)).numpy().ravel()\n",
    "    pd.DataFrame( np.array([test_flist, preds]).T, columns=['path', 'pred']).to_csv('{}/VIT_{}.csv'.format(RESULT_PATH, fid),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "# Cross validation\n",
    "for fid in range(1,6):\n",
    "    vit(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_model = tf.keras.Model(model.input, model.layers[-8].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = []\n",
    "for t in test_dataset:\n",
    "    tes = t\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sub_model.predict([ tes[0] ] )\n",
    "k = 16\n",
    "print(test_flist[k])\n",
    "t = res[1][k][5, 0, :].reshape(2, 2, 10)\n",
    "plt.imshow(cv.cvtColor(cv.imread(test_flist[k]), cv.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t.reshape((-1, 10)).sum(axis=0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "vivit",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
